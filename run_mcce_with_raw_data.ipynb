{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "n_test = 100\n",
    "K = 1000\n",
    "\n",
    "import torch\n",
    "\n",
    "from mcce import MCCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using Python-MIP package version 1.12.0 [model.py <module>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/ipykernel_launcher.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/ipykernel_launcher.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from carla.data.catalog import OnlineCatalog\n",
    "import pandas as pd\n",
    "\n",
    "train_path = \"/nr/samba/user/anr/pkg/MCCE/Datasets/Adult/adult.data\"\n",
    "test_path = \"/nr/samba/user/anr/pkg/MCCE/Datasets/Adult/adult.test\"\n",
    "train = pd.read_csv(train_path, sep=\", \", header=None, names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
    "test = pd.read_csv(test_path, skiprows=1, sep=\", \", header=None, names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
    "\n",
    "df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "# df = train\n",
    "\n",
    "# df.to_csv(\"/nr/samba/user/anr/pkg/MCCE/Datasets/Adult/Adult_train_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>...</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt  education  education-num  ... capital-gain  \\\n",
       "0   39  State-gov   77516  Bachelors             13  ...         2174   \n",
       "\n",
       "  capital-loss hours-per-week native-country income  \n",
       "0            0             40  United-States  <=50K  \n",
       "\n",
       "[1 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'>50K': '>50K', '>50K.': '>50K', '<=50K': '<=50K', '<=50K.': '<=50K'}\n",
    "\n",
    "df['income'] = [mapping[item] for item in df['income']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'workclass'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "mapping = d.to_dict()\n",
    "df[feature] = [mapping[item] for item in df[feature]]\n",
    "\n",
    "feature = 'education'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "mapping = d.to_dict()\n",
    "df[feature] = [mapping[item] for item in df[feature]]\n",
    "\n",
    "feature = 'marital-status'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "mapping = d.to_dict()\n",
    "df[feature] = [mapping[item] for item in df[feature]]\n",
    "\n",
    "feature = 'occupation'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "mapping = d.to_dict()\n",
    "df[feature] = [mapping[item] for item in df[feature]]\n",
    "\n",
    "feature = 'relationship'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "mapping = d.to_dict()\n",
    "df[feature] = [mapping[item] for item in df[feature]]\n",
    "\n",
    "feature = 'sex'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "mapping = d.to_dict()\n",
    "df[feature] = [mapping[item] for item in df[feature]]\n",
    "\n",
    "feature = 'race'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "mapping = d.to_dict()\n",
    "df[feature] = [mapping[item] for item in df[feature]]\n",
    "\n",
    "feature = 'native-country'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "mapping = d.to_dict()\n",
    "df[feature] = [mapping[item] for item in df[feature]]\n",
    "\n",
    "feature = 'income'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "mapping = d.to_dict()\n",
    "df[feature] = [mapping[item] for item in df[feature]]\n",
    "\n",
    "df.drop(['education'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age (74,)\n",
      "workclass (4,)\n",
      "fnlwgt (28523,)\n",
      "education-num (16,)\n",
      "marital-status (4,)\n",
      "occupation (4,)\n",
      "relationship (4,)\n",
      "race (4,)\n",
      "sex (2,)\n",
      "capital-gain (123,)\n",
      "capital-loss (99,)\n",
      "hours-per-week (96,)\n",
      "native-country (4,)\n",
      "income (2,)\n"
     ]
    }
   ],
   "source": [
    "for x in df.columns:\n",
    "    print(x, df.groupby(x).size().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/nr/samba/user/anr/pkg/MCCE/Datasets/Adult/train_not_normalized_data_from_carla.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous = ['age',\n",
    "#     'fnlwgt',\n",
    "#     'education-num',\n",
    "#     'capital-gain',\n",
    "#     'hours-per-week',\n",
    "#     'capital-loss']\n",
    "\n",
    "# categorical = ['marital-status',\n",
    "# 'native-country',\n",
    "# 'occupation',\n",
    "# 'race',\n",
    "# 'relationship',\n",
    "# 'sex',\n",
    "# 'workclass']\n",
    "\n",
    "# from sklearn import preprocessing\n",
    "# scaler = preprocessing.MinMaxScaler().fit(df[continuous])\n",
    "# df_scaled = scaler.transform(df[continuous])\n",
    "# df_scaled = pd.DataFrame(df_scaled)\n",
    "# df_scaled.columns = continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# enc = OneHotEncoder(drop='first')\n",
    "# enc.fit(df[categorical])\n",
    "\n",
    "# df_one_hot_encoded = pd.DataFrame(enc.transform(df[categorical]).toarray(), columns=enc.get_feature_names(categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled = pd.concat([df_scaled, df_one_hot_encoded, df['income']], axis=1) # axis = 0 is index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled.sex_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled.to_csv(\"/nr/samba/user/anr/pkg/MCCE/Datasets/Adult/for_CsvCatalog.csv\", index=False)\n",
    "\n",
    "# pd.read_csv(\"/nr/samba/user/anr/pkg/MCCE/Datasets/Adult/train_not_normalized_data_from_carla.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from carla.data.catalog import CsvCatalog\n",
    "\n",
    "continuous = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"hours-per-week\", \"capital-loss\"]\n",
    "categorical = [\"marital-status\", \"native-country\", \"occupation\", \"race\", \"relationship\", \"sex\", \"workclass\"]\n",
    "# categorical = enc.get_feature_names(categorical)\n",
    "immutable = [\"age\", \"sex\"]\n",
    "\n",
    "dataset = CsvCatalog(file_path=\"/nr/samba/user/anr/pkg/MCCE/Datasets/Adult/train_not_normalized_data_from_carla.csv\",\n",
    "                     continuous=continuous,\n",
    "                     categorical=categorical,\n",
    "                     immutables=immutable,\n",
    "                     target='income',\n",
    "                    #  scaling_method=None,\n",
    "                     encoding_method=\"OneHot_drop_first\",\n",
    "                     )\n",
    "\n",
    "# dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.continuous\n",
    "dataset.categorical\n",
    "dataset.df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from carla.models.catalog import MLModelCatalog\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "ml_model = MLModelCatalog(\n",
    "        dataset, \n",
    "        model_type=\"ann\", \n",
    "        load_online=False, \n",
    "        backend=\"pytorch\"\n",
    "    )\n",
    "\n",
    "\n",
    "ml_model.train(\n",
    "learning_rate=0.002,\n",
    "epochs=20,\n",
    "batch_size=1024,\n",
    "hidden_size=[18, 9, 3],\n",
    "force_train=True, # don't forget to add this or it might load an older model from disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pred = ml_model.predict_proba(dataset.df_test)\n",
    "pred = [row[1] for row in pred]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(dataset.df_test[dataset.target], pred, pos_label=1)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# temp = dataset.df_test.copy()\n",
    "# temp['pred'] = pred\n",
    "# temp = pd.concat([pd.DataFrame(scaler.inverse_transform(temp[continuous]), columns = continuous, index = temp.index), temp[categorical]], axis=1)\n",
    "# temp.sort_index()\n",
    "#temp.sort_values(temp.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from carla.models.negative_instances import predict_negative_instances\n",
    "\n",
    "factuals = predict_negative_instances(ml_model, dataset.df)\n",
    "test_factual = factuals.iloc[:n_test]\n",
    "# test_factual_inverse = dataset.inverse_transform(test_factual)\n",
    "\n",
    "y_col = dataset.target\n",
    "features_and_response = dataset.df.columns\n",
    "cont_feat = dataset.continuous\n",
    "cat_feat = [x for x in features_and_response if x not in cont_feat] #  these have new names since encode_normalize_order_factuals()\n",
    "\n",
    "fixed_features = ['age', 'sex_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create dtypes for MCCE()\n",
    "dtypes = dict([(x, \"float\") for x in cont_feat])\n",
    "for x in cat_feat:\n",
    "    dtypes[x] = \"category\"\n",
    "df = (dataset.df).astype(dtypes)\n",
    "\n",
    "import time\n",
    "start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Fit MCCE object\n",
    "print(\"Fitting MCCE model...\")\n",
    "mcce = MCCE(fixed_features=['age', 'sex_1'], immutables=['age', 'sex_1'], \\\n",
    "    model=ml_model, seed=1, continuous=dataset.continuous, categorical=dataset.categorical)\n",
    "mcce.fit(df.drop(y_col, axis=1), dtypes)\n",
    "\n",
    "print(\"Generating counterfactuals with MCCE...\")\n",
    "synth_df = mcce.generate(test_factual.drop(y_col, axis=1), k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.inverse_transform(dataset.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = df\n",
    "synth = synth_df\n",
    "test = test_factual\n",
    "response = y_col\n",
    "inverse_transform = dataset.inverse_transform\n",
    "cutoff = 0.5\n",
    "# Predict response of generated data\n",
    "synth[response] = ml_model.predict(synth)\n",
    "synth_positive = synth[synth[response]>=cutoff] # drop negative responses\n",
    "\n",
    "\n",
    "# Duplicate original test observations N times where N is number of positive counterfactuals\n",
    "n_counterfactuals = synth_positive.groupby(synth_positive.index).size()\n",
    "n_counterfactuals = pd.DataFrame(n_counterfactuals, columns = ['N'])\n",
    "\n",
    "test_repeated = test.copy()\n",
    "\n",
    "test_repeated = test_repeated.join(n_counterfactuals)\n",
    "test_repeated.dropna(inplace = True)\n",
    "\n",
    "test_repeated = test_repeated.reindex(test_repeated.index.repeat(test_repeated.N))\n",
    "test_repeated.drop(['N'], axis=1, inplace=True)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "synth=synth_positive\n",
    "test=test_repeated\n",
    "\n",
    "features = synth.columns.to_list()\n",
    "features.remove(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_metrics = synth.copy()\n",
    "synth.sort_index(inplace=True)\n",
    "\n",
    "cols = data.columns\n",
    "cols.drop(response)\n",
    "\n",
    "feas_results = []\n",
    "nbrs = NearestNeighbors(n_neighbors=5).fit(synth[cols].values)\n",
    "\n",
    "for i, row in synth[cols].iterrows():\n",
    "    knn = nbrs.kneighbors(row.values.reshape((1, -1)), 5, return_distance=True)[0]\n",
    "    \n",
    "    feas_results.append(np.mean(knn))\n",
    "\n",
    "synth_metrics['feasibility'] = feas_results\n",
    "\n",
    "synth_metrics['success'] = 1\n",
    "\n",
    "# 6) Success\n",
    "synth_metrics['success'] = 1\n",
    "synth.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_encoded = []\n",
    "for x in dataset.df.columns:\n",
    "    if x not in dataset.continuous:\n",
    "        if x not in dataset.target:\n",
    "            categorical_encoded.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "len(synth.index.unique())\n",
    "test.loc[1][features].iloc[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth.iloc[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Distance: Sparsity and Euclidean distance\n",
    "factual = test[features].sort_index().to_numpy()\n",
    "counterfactuals = synth[features].sort_index().to_numpy()\n",
    "\n",
    "cfs_continuous = synth[dataset.continuous].sort_index().to_numpy()\n",
    "cfs_categorical = synth[categorical_encoded].sort_index().to_numpy()\n",
    "\n",
    "factual_continuous = test[dataset.continuous].sort_index().to_numpy()\n",
    "factual_categorical = test[categorical_encoded].sort_index().to_numpy()\n",
    "\n",
    "delta_cont = factual_continuous - cfs_continuous\n",
    "delta_cat = factual_categorical - cfs_categorical\n",
    "\n",
    "delta_cat = np.where(np.abs(delta_cat) > 0, 1, 0)\n",
    "\n",
    "delta = np.concatenate((delta_cont, delta_cat), axis=1)\n",
    "d1 = np.sum(np.invert(np.isclose(delta, np.zeros_like(delta), atol=1e-5)), axis=1, dtype=float).tolist() # sparsity\n",
    "d2 = np.sum(np.abs(delta), axis=1, dtype=float).tolist() # manhatten distance\n",
    "d3 = np.sum(np.square(np.abs(delta)), axis=1, dtype=np.float).tolist() # euclidean distance\n",
    "\n",
    "synth_metrics['L0'] = d1\n",
    "synth_metrics['L1'] = d2\n",
    "synth_metrics['L2'] = d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_decoded_cfs = pd.DataFrame(scaler.inverse_transform(synth[continuous]), columns=continuous)\n",
    "# df_decoded_cfs.index = synth.index\n",
    "\n",
    "# df_decoded_cfs = pd.concat([df_decoded_cfs, synth[categorical]], axis=1)\n",
    "# df_decoded_cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform(df, continuous, categorical, scaler):\n",
    "#     df_transform = scaler.transform(df[continuous])\n",
    "#     df_transform = pd.DataFrame(df_transform, columns=continuous, index=df.index)\n",
    "#     return pd.concat([df_transform, df[categorical]], axis=1)\n",
    "\n",
    "\n",
    "# def inverse_transform(df, continuous, categorical, scaler):\n",
    "#     df_transform = scaler.inverse_transform(df[continuous])\n",
    "#     df_transform = pd.DataFrame(df_transform, columns=continuous, index=df.index)\n",
    "#     return pd.concat([df_transform, df[categorical]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "\n",
    "df_decoded_cfs = dataset.inverse_transform(synth)\n",
    "\n",
    "df_factuals = dataset.inverse_transform(test)\n",
    "\n",
    "# check continuous using np.isclose to allow for very small numerical differences\n",
    "cfs_continuous_immutable = df_decoded_cfs[\n",
    "    intersection(dataset.continuous, fixed_features)\n",
    "]\n",
    "\n",
    "factual_continuous_immutable = df_factuals[\n",
    "    intersection(dataset.continuous, dataset.immutables)\n",
    "]\n",
    "\n",
    "continuous_violations = np.invert(\n",
    "    np.isclose(cfs_continuous_immutable, factual_continuous_immutable)\n",
    ")\n",
    "continuous_violations = np.sum(continuous_violations, axis=1).reshape(\n",
    "    (-1, 1)\n",
    ") \n",
    "\n",
    "# check categorical by boolean comparison\n",
    "cfs_categorical_immutable = df_decoded_cfs[\n",
    "    intersection(dataset.categorical, dataset.immutables)\n",
    "]\n",
    "# print(cfs_categorical_immutable)\n",
    "factual_categorical_immutable = df_factuals[\n",
    "    intersection(dataset.categorical, dataset.immutables)\n",
    "]\n",
    "\n",
    "\n",
    "cfs_categorical_immutable.sort_index(inplace=True)\n",
    "factual_categorical_immutable.sort_index(inplace=True)\n",
    "cfs_categorical_immutable.index.name = None\n",
    "\n",
    "categorical_violations = cfs_categorical_immutable != factual_categorical_immutable\n",
    "categorical_violations = np.sum(categorical_violations.values, axis=1).reshape(\n",
    "    (-1, 1)\n",
    ")  # sum over features\n",
    "\n",
    "synth_metrics['violation'] = continuous_violations + categorical_violations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = synth_metrics.copy()\n",
    "results_sparse = pd.DataFrame(columns=results.columns)\n",
    "\n",
    "for idx in list(set(results.index)):\n",
    "    idx_df = results.loc[idx]\n",
    "    if(isinstance(idx_df, pd.DataFrame)): # If you have multiple rows\n",
    "        sparse = min(idx_df.L0) # 1) find least # features changed\n",
    "        sparse_df = idx_df[idx_df.L0 == sparse] \n",
    "        closest = min(sparse_df.L2) # find smallest Gower distance\n",
    "        close_df = sparse_df[sparse_df.L2 == closest]\n",
    "\n",
    "        if(close_df.shape[0]>1):\n",
    "            highest_feasibility = max(close_df.feasibility) #  3) find most feasible\n",
    "            close_df = close_df[close_df.feasibility == highest_feasibility].head(1)\n",
    "\n",
    "    else: # if you have only one row - return that row\n",
    "        close_df = idx_df.to_frame().T\n",
    "        \n",
    "    results_sparse = pd.concat([results_sparse, close_df], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_sparse[['L0', 'L1', 'L2', 'feasibility', 'violation', 'success']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.inverse_transform(results_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sparse.to_csv(f\"/nr/samba/user/anr/pkg/MCCE_Python/Results/adult_mcce_results_raw_data_k_{K}_n_{n_test}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcce_raw = pd.read_csv(f\"/nr/samba/user/anr/pkg/MCCE_Python/Results/adult_mcce_results_raw_data_k_10000_n_100.csv\", index_col=0)\n",
    "# mcce_raw.dtypes\n",
    "# mcce_raw.L0.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_preds = ml_model.predict_proba(mcce_raw)\n",
    "new_preds = []\n",
    "for x in orig_preds:\n",
    "    new_preds.append(x[1])\n",
    "\n",
    "results_inverse = dataset.inverse_transform(mcce_raw)\n",
    "\n",
    "\n",
    "results_inverse['pred'] = new_preds\n",
    "# results_inverse[['age', 'workclass', 'fnlwgt', 'education-num', 'marital-status', 'relationship', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# results_inverse.to_csv(\"/nr/samba/user/anr/pkg/MCCE_Python/Results/adult_mcce_results_raw_data_k_10000_n_100_inverse_transform.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_raw =  dataset.inverse_transform(test_factual)\n",
    "\n",
    "orig_preds = ml_model.predict_proba(test_factual)\n",
    "new_preds = []\n",
    "for x in orig_preds:\n",
    "    new_preds.append(x[1])\n",
    "\n",
    "true_raw['pred'] = new_preds\n",
    "\n",
    "true_raw.to_csv(\"/nr/samba/user/anr/pkg/MCCE_Python/Results/adult_raw_data_n_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_inverse = pd.read_csv(\"/nr/samba/user/anr/pkg/MCCE_Python/Results/adult_mcce_results_raw_data_k_10000_n_100_inverse_transform.csv\", index_col=0)\n",
    "\n",
    "\n",
    "print(results_inverse.L0.mean())\n",
    "print(results_inverse.L2.mean())\n",
    "print(results_inverse.feasibility.mean())\n",
    "print(results_inverse.violation.mean())\n",
    "print(results_inverse.success.mean())\n",
    "print(results_inverse.shape[0])\n",
    "print(results_inverse['time (seconds)'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_inverse[['L0', 'L2', 'feasibility', 'violation', 'success', 'time (seconds)']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/nr/samba/user/anr/pkg/MCCE/Datasets/Adult/adult.data\"\n",
    "test_path = \"/nr/samba/user/anr/pkg/MCCE/Datasets/Adult/adult.test\"\n",
    "train = pd.read_csv(train_path, sep=\", \", header=None, names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
    "test = pd.read_csv(test_path, skiprows=1, sep=\", \", header=None, names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
    "df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_inverse['method'] = 'MCCE'\n",
    "true_raw['method'] = 'Original'\n",
    "temp = pd.concat([results_inverse, true_raw])\n",
    "\n",
    "cols = ['method', 'age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', \\\n",
    "       'hours-per-week', 'marital-status', 'native-country', \\\n",
    "       'occupation', 'race', 'relationship', 'sex', 'workclass']\n",
    "\n",
    "to_write = temp[cols].loc[[1, 31, 122, 124]].sort_index()\n",
    "to_write.columns = cols\n",
    "# to_write.sort_values(['Method'], inplace=True, ascending=False)\n",
    "\n",
    "to_write.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'workclass'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "\n",
    "mapping = d.to_dict()\n",
    "dct = {v: k for k, v in mapping.items()}\n",
    "\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'marital-status'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "mapping = d.to_dict()\n",
    "dct = {v: k for k, v in mapping.items()}\n",
    "\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'occupation'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "mapping = d.to_dict()\n",
    "dct = {v: k for k, v in mapping.items()}\n",
    "\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'relationship'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "mapping = d.to_dict()\n",
    "dct = {v: k for k, v in mapping.items()}\n",
    "\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'sex'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "mapping = d.to_dict()\n",
    "dct = {v: k for k, v in mapping.items()}\n",
    "\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'race'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "mapping = d.to_dict()\n",
    "dct = {v: k for k, v in mapping.items()}\n",
    "\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'native-country'\n",
    "d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "for i, ind in enumerate(d):\n",
    "    if i <= 3:\n",
    "        d[i] = i\n",
    "    else:\n",
    "        d[i] = 3\n",
    "mapping = d.to_dict()\n",
    "dct = {v: k for k, v in mapping.items()}\n",
    "\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_write\n",
    "# feature = 'workclass'\n",
    "# [dct[item] for item in to_write[feature]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'marital-status'\n",
    "dct = {'Married-civ-spouse': 'MCS', 'Never-married': 'NM', 'Divorced': 'D', 'Married-AF-spouse': 'MAFS'}\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'native-country'\n",
    "dct = {'United-States': 'US', 'Holand-Netherlands': 'HS'}\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'occupation'\n",
    "dct = {'Exec-managerial': 'EM', 'Armed-Forces': 'AF', 'Prof-specialty': 'P'}\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'race'\n",
    "dct = {'White': 'W', 'Black': 'B', 'Asian-Pac-Islander': 'API'}\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "\n",
    "feature = 'relationship'\n",
    "dct = {'Husband': 'H', 'Own-child': 'OC'}\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'sex'\n",
    "dct = {'Male': 'M'}\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "\n",
    "feature = 'workclass'\n",
    "dct = {'Self-emp-not-inc': 'SENI', 'Private': 'P', 'Never-worked': 'NW'}\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_write.head(1)\n",
    "print(to_write.to_latex(index=False, float_format=\"%.0f\", ))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea38de303447ace9d448c28089670fa84711b12cac6767c435896f96584513e1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('carla_github')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
