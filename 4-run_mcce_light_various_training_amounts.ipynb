{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from carla.data.catalog import OnlineCatalog\n",
    "from carla.models.catalog import MLModelCatalog\n",
    "from carla.models.negative_instances import predict_negative_instances, predict_label\n",
    "\n",
    "import torch\n",
    "import time\n",
    "            \n",
    "from mcce import MCCE\n",
    "\n",
    "## FOR EACH DATA SET you have to adjust n below - \n",
    "## for adult and gmc, I use 100, 1000, 10000 and the size of the data set\n",
    "## for compas, I use 100, 1000, 5000, and the size of the data set \n",
    "\n",
    "data_name = \"adult\"\n",
    "data_name = 'give_me_some_credit'\n",
    "data_name = 'compas'\n",
    "n_test = 100\n",
    "seed = 1\n",
    "\n",
    "dataset = OnlineCatalog(data_name)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "ml_model = MLModelCatalog(\n",
    "        dataset, \n",
    "        model_type=\"ann\", \n",
    "        load_online=False, \n",
    "        backend=\"pytorch\"\n",
    "        )\n",
    "if data_name == 'adult':\n",
    "    ml_model.train(\n",
    "    learning_rate=0.002,\n",
    "    epochs=20,\n",
    "    batch_size=1024,\n",
    "    hidden_size=[18, 9, 3],\n",
    "    force_train=True, # don't forget to add this or it might load an older model from disk\n",
    "    )\n",
    "elif data_name == 'give_me_some_credit':\n",
    "    ml_model.train(\n",
    "    learning_rate=0.002,\n",
    "    epochs=20,\n",
    "    batch_size=2048,\n",
    "    hidden_size=[18, 9, 3],\n",
    "    force_train=True, # don't forget to add this or it might load an older model from disk\n",
    "    )\n",
    "elif data_name == 'compas':\n",
    "    ml_model.train(\n",
    "    learning_rate=0.002,\n",
    "    epochs=25,\n",
    "    batch_size=25,\n",
    "    hidden_size=[18, 9, 3],\n",
    "    force_train=True, # don't forget to add this or it might load an older model from disk\n",
    "    )\n",
    "\n",
    "# (2) Find unhappy customers and choose which ones to make counterfactuals for\n",
    "\n",
    "factuals = predict_negative_instances(ml_model, dataset.df)\n",
    "test_factual = factuals.iloc[:n_test]\n",
    "\n",
    "y_col = dataset.target\n",
    "cont_feat = dataset.continuous\n",
    "\n",
    "cat_feat = dataset.categorical\n",
    "cat_feat_encoded = dataset.encoder.get_feature_names(dataset.categorical)\n",
    "\n",
    "if data_name == 'adult': \n",
    "    fixed_features = ['age', 'sex_Male']\n",
    "elif data_name == 'give_me_some_credit':\n",
    "    fixed_features = ['age']\n",
    "elif data_name == 'compas':\n",
    "    fixed_features = ['age', 'sex_Male', 'race_Other']\n",
    "\n",
    "#  Create dtypes for MCCE()\n",
    "dtypes = dict([(x, \"float\") for x in cont_feat])\n",
    "for x in cat_feat_encoded:\n",
    "    dtypes[x] = \"category\"\n",
    "df = (dataset.df).astype(dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_indices = test_factual.index.to_list()\n",
    "all_indices = dataset.df.index.to_list()\n",
    "possible_train_indices = set(factual_indices) ^ set(all_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_name == 'adult': \n",
    "    n_list = [100, 1000, 10000, len(possible_train_indices)]\n",
    "elif data_name == 'give_me_some_credit':\n",
    "    n_list = [100, 1000, 10000, 50000, len(possible_train_indices)]\n",
    "elif data_name == 'compas':\n",
    "    n_list = [100, 1000, 5000, len(possible_train_indices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we don't fit the typical MCCE metho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "results = []\n",
    "for n in n_list:\n",
    "\n",
    "    if n == len(possible_train_indices): # if the whole data set\n",
    "\n",
    "        dim = dataset.df.shape[0]\n",
    "        print(dim)\n",
    "        random.seed(s)\n",
    "        rows = random.sample(possible_train_indices, n)\n",
    "        rows = np.sort(rows)\n",
    "\n",
    "        positives = (df.loc[rows]).copy()\n",
    "        positives[\"y\"] = predict_label(ml_model, positives)\n",
    "        positives = positives[positives[\"y\"] == 1]\n",
    "        positives = positives.drop(\"y\", axis=\"columns\")\n",
    "\n",
    "        positives = dataset.inverse_transform(positives)\n",
    "        test_factual_inverse = dataset.inverse_transform(test_factual)\n",
    "        test_factual_inverse.index.name = 'test'\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        synth = pd.merge(test_factual_inverse.reset_index()[dataset.immutables + ['test']], positives, on = dataset.immutables).set_index(['test']) # 'train',\n",
    "        synth = dataset.transform(synth) # go from normal to one-hot encoded\n",
    "\n",
    "        mcce = MCCE(fixed_features=fixed_features, continuous=dataset.continuous, categorical=dataset.categorical,\\\n",
    "            model=ml_model, seed=1, catalog=dataset.catalog)\n",
    "\n",
    "        mcce.fit(df.drop(y_col, axis=1), dtypes)\n",
    "\n",
    "        mcce.postprocess(data=df, synth=synth, test=test_factual, response=y_col, \\\n",
    "            inverse_transform=dataset.inverse_transform, cutoff=0.5)\n",
    "\n",
    "        timing = time.time() - start\n",
    "\n",
    "        mcce.results_sparse['time (seconds)'] = timing\n",
    "\n",
    "        results.append([mcce.results_sparse.L0.mean(), mcce.results_sparse.L2.mean(), mcce.results_sparse.feasibility.mean(), mcce.results_sparse.violation.mean(), mcce.results_sparse.shape[0], timing, n, s])\n",
    "    else:\n",
    "        for s in range(5): # range(5):\n",
    "\n",
    "            dim = dataset.df.shape[0]\n",
    "            print(dim)\n",
    "            random.seed(s)\n",
    "            rows = random.sample(possible_train_indices, n)\n",
    "            rows = np.sort(rows)\n",
    "\n",
    "            positives = (df.loc[rows]).copy()\n",
    "            positives[\"y\"] = predict_label(ml_model, positives)\n",
    "            positives = positives[positives[\"y\"] == 1]\n",
    "            positives = positives.drop(\"y\", axis=\"columns\")\n",
    "\n",
    "            positives = dataset.inverse_transform(positives)\n",
    "            test_factual_inverse = dataset.inverse_transform(test_factual)\n",
    "            test_factual_inverse.index.name = 'test'\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            synth = pd.merge(test_factual_inverse.reset_index()[dataset.immutables + ['test']], positives, on = dataset.immutables).set_index(['test']) # 'train',\n",
    "            synth = dataset.transform(synth) # go from normal to one-hot encoded\n",
    "\n",
    "            mcce = MCCE(fixed_features=fixed_features, continuous=dataset.continuous, categorical=dataset.categorical,\\\n",
    "            model=ml_model, seed=1, catalog=dataset.catalog)\n",
    "\n",
    "            mcce.fit(df.drop(y_col, axis=1), dtypes)\n",
    "\n",
    "            mcce.postprocess(data=df, synth=synth, test=test_factual, response=y_col, \\\n",
    "                inverse_transform=dataset.inverse_transform, cutoff=0.5)\n",
    "\n",
    "            timing = time.time() - start\n",
    "\n",
    "            mcce.results_sparse['time (seconds)'] = timing\n",
    "\n",
    "            results.append([mcce.results_sparse.L0.mean(), mcce.results_sparse.L2.mean(), mcce.results_sparse.feasibility.mean(), mcce.results_sparse.violation.mean(), mcce.results_sparse.shape[0], timing, n, s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L0</th>\n",
       "      <th>L2</th>\n",
       "      <th>feasibility</th>\n",
       "      <th>violation</th>\n",
       "      <th>NCE</th>\n",
       "      <th>timing</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ntest</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2.562510</td>\n",
       "      <td>0.801110</td>\n",
       "      <td>0.072443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.4</td>\n",
       "      <td>0.512758</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1.816698</td>\n",
       "      <td>0.213898</td>\n",
       "      <td>0.047730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.2</td>\n",
       "      <td>1.409829</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>1.514000</td>\n",
       "      <td>0.096621</td>\n",
       "      <td>0.022430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.619240</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6072</th>\n",
       "      <td>1.490000</td>\n",
       "      <td>0.080221</td>\n",
       "      <td>0.020698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5.532952</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             L0        L2  feasibility  violation    NCE    timing  seed\n",
       "Ntest                                                                   \n",
       "100    2.562510  0.801110     0.072443        0.0   59.4  0.512758   2.0\n",
       "1000   1.816698  0.213898     0.047730        0.0   98.2  1.409829   2.0\n",
       "5000   1.514000  0.096621     0.022430        0.0  100.0  4.619240   2.0\n",
       "6072   1.490000  0.080221     0.020698        0.0  100.0  5.532952   4.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2 = pd.DataFrame(results, columns=['L0', 'L2', 'feasibility', 'violation', 'NCE', 'timing', 'Ntest', 'seed'])\n",
    "results2\n",
    "results2.groupby(['Ntest']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = mcce.results_sparse\n",
    "temp.to_csv(f\"Results/{data_name}_baseline_results_n_{n_test}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positives = dataset.df.copy()\n",
    "# positives[\"y\"] = predict_label(ml_model, positives)\n",
    "# positives = positives[positives[\"y\"] == 1]\n",
    "# positives = positives.drop(\"y\", axis=\"columns\")\n",
    "\n",
    "# positives = dataset.inverse_transform(positives)\n",
    "# test_factual_inverse = dataset.inverse_transform(test_factual)\n",
    "# test_factual_inverse.index.name = 'test'\n",
    "\n",
    "# import time\n",
    "# start = time.time()\n",
    "\n",
    "# synth = pd.merge(test_factual_inverse.reset_index()[dataset.immutables + ['test']], positives, on = dataset.immutables).set_index(['test']) # 'train',\n",
    "# synth = dataset.transform(synth) # go from normal to one-hot encoded\n",
    "\n",
    "# from mcce import MCCE\n",
    "\n",
    "# mcce = MCCE(fixed_features=fixed_features, immutables=immutables, model=ml_model, continuous=dataset.continuous, categorical=dataset.categorical)\n",
    "\n",
    "# mcce.fit(df.drop(dataset.target, axis=1), dtypes)\n",
    "\n",
    "# mcce.postprocess(data=dataset.df, synth=synth, test=test_factual, response=y_col, \\\n",
    "#     transform=None, inverse_transform=dataset.inverse_transform, cutoff=0.5)\n",
    "\n",
    "# timing = time.time() - start\n",
    "# print(timing)\n",
    "\n",
    "# mcce.results_sparse['time (seconds)'] = timing\n",
    "\n",
    "# results.append([mcce.results_sparse.L0.mean(), mcce.results_sparse.L2.mean(), mcce.results_sparse.feasibility.mean(), mcce.results_sparse.violation.mean(), mcce.results_sparse.shape[0], timing, 48000, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synth.index.name = None\n",
    "# synth\n",
    "# test_factual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=dataset.df\n",
    "# test=test_factual\n",
    "# response=y_col\n",
    "# transform=None\n",
    "# inverse_transform=dataset.inverse_transform\n",
    "# cutoff=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict response of generated data\n",
    "# synth[response] = ml_model.predict(synth)\n",
    "# synth_positive = synth[synth[response]>=cutoff] # drop negative responses\n",
    "\n",
    "\n",
    "# # Duplicate original test observations N times where N is number of positive counterfactuals\n",
    "# n_counterfactuals = synth_positive.groupby(synth_positive.index).size()\n",
    "# n_counterfactuals = pd.DataFrame(n_counterfactuals, columns = ['N'])\n",
    "\n",
    "# test_repeated = test.copy()\n",
    "\n",
    "# test_repeated = test_repeated.join(n_counterfactuals)\n",
    "# test_repeated.dropna(inplace = True)\n",
    "\n",
    "# test_repeated = test_repeated.reindex(test_repeated.index.repeat(test_repeated.N))\n",
    "# test_repeated.drop(['N'], axis=1, inplace=True)\n",
    "\n",
    "# test = test_repeated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_repeated.sort_index(inplace=True)\n",
    "# test_repeated.iloc[804]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = synth.columns.to_list()\n",
    "# features.remove(response)\n",
    "\n",
    "# synth_metrics = synth.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synth.sort_index(inplace=True)\n",
    "# synth.iloc[804:806]\n",
    "# .iloc[804:806]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def intersection(lst1, lst2):\n",
    "#     return list(set(lst1) & set(lst2))\n",
    "\n",
    "# df_decoded_cfs = inverse_transform(synth.copy())\n",
    "\n",
    "# df_factuals = inverse_transform(test.copy())\n",
    "\n",
    "# # check continuous using np.isclose to allow for very small numerical differences\n",
    "# cfs_continuous_immutable = df_decoded_cfs[\n",
    "#     intersection(dataset.continuous, fixed_features)\n",
    "# ]\n",
    "# factual_continuous_immutable = df_factuals[\n",
    "#     intersection(dataset.continuous, dataset.immutables)\n",
    "# ]\n",
    "# # print(cfs_continuous_immutable)\n",
    "# print(factual_continuous_immutable.shape)\n",
    "\n",
    "# continuous_violations = np.invert(\n",
    "#     np.isclose(cfs_continuous_immutable, factual_continuous_immutable)\n",
    "# )\n",
    "# continuous_violations = np.sum(continuous_violations, axis=1).reshape(\n",
    "#     (-1, 1)\n",
    "# )  # sum over features\n",
    "\n",
    "# cfs_categorical_immutable = df_decoded_cfs[\n",
    "#     intersection(dataset.categorical, dataset.immutables)\n",
    "# ]\n",
    "# factual_categorical_immutable = df_factuals[\n",
    "#     intersection(dataset.categorical, dataset.immutables)\n",
    "# ]\n",
    "\n",
    "# cfs_categorical_immutable.sort_index(inplace=True)\n",
    "# factual_categorical_immutable.sort_index(inplace=True)\n",
    "# cfs_categorical_immutable.index.name = None\n",
    "\n",
    "# categorical_violations = cfs_categorical_immutable != factual_categorical_immutable\n",
    "\n",
    "# categorical_violations = np.sum(categorical_violations.values, axis=1).reshape(\n",
    "#             (-1, 1)\n",
    "#         )\n",
    "\n",
    "\n",
    "# factual_categorical_immutable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah = []\n",
    "# for x in (continuous_violations + categorical_violations):\n",
    "#     blah.append(x[0])\n",
    "# np.mean(blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, x in enumerate(blah):\n",
    "#     if x == 1:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcce.results_sparse.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_factual.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if you want to find out which data point the test observation \"found\" in the training data\n",
    "\n",
    "# idx = 1\n",
    "# temp = mcce.results_sparse.iloc[idx:(idx + 1)]\n",
    "\n",
    "# feat = ['age', 'fnlwgt', 'education-num', 'capital-gain']\n",
    "\n",
    "# to_show = pd.merge(temp[feat], dataset.df.reset_index(), on = feat).set_index('index')\n",
    "\n",
    "# to_show.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(mcce.results_sparse.violation.mean())\n",
    "# print(mcce.results_sparse.L0.mean())\n",
    "# print(mcce.results_sparse.L1.mean())\n",
    "# print(mcce.results_sparse.L2.mean())\n",
    "\n",
    "# print(mcce.results_sparse.feasibility.mean())\n",
    "# print(mcce.results_sparse.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcce.results_sparse"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea38de303447ace9d448c28089670fa84711b12cac6767c435896f96584513e1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('carla_github')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
