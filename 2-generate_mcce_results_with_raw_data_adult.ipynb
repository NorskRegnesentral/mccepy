{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from mcce import MCCE\n",
    "n_test = 100\n",
    "K = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw train/test of Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/ipykernel_launcher.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/ipykernel_launcher.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_path = \"Data/adult.data\"\n",
    "test_path = \"Data/adult.test\"\n",
    "train = pd.read_csv(train_path, sep=\", \", header=None, \\\n",
    "    names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', \\\n",
    "        'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
    "test = pd.read_csv(test_path, skiprows=1, sep=\", \", header=None, \\\n",
    "    names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', \\\n",
    "        'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
    "\n",
    "df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "df = df.drop(['education'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess categorical features to have 4 levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\">50K\": \">50K\", \">50K.\": \">50K\", \"<=50K\": \"<=50K\", \"<=50K.\": \"<=50K\"}\n",
    "\n",
    "df[\"income\"] = [mapping[item] for item in df[\"income\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in [\"workclass\", \"marital-status\", \"occupation\", \"relationship\", \\\n",
    "    \"sex\", \"race\", \"native-country\", \"income\"]:\n",
    "    d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "    for i, ind in enumerate(d):\n",
    "        if i <= 3:\n",
    "            d[i] = i\n",
    "        else:\n",
    "            d[i] = 3\n",
    "    mapping = d.to_dict()\n",
    "    df[feature] = [mapping[item] for item in df[feature]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Data/train_not_normalized_data_from_carla.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data in using CARLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using Python-MIP package version 1.12.0 [model.py <module>]\n"
     ]
    }
   ],
   "source": [
    "from carla.data.catalog import CsvCatalog\n",
    "\n",
    "continuous = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"hours-per-week\", \"capital-loss\"]\n",
    "categorical = [\"marital-status\", \"native-country\", \"occupation\", \"race\", \"relationship\", \"sex\", \"workclass\"]\n",
    "immutable = [\"age\", \"sex\"]\n",
    "\n",
    "dataset = CsvCatalog(file_path=\"Data/train_not_normalized_data_from_carla.csv\",\n",
    "                     continuous=continuous,\n",
    "                     categorical=categorical,\n",
    "                     immutables=immutable,\n",
    "                     target=\"income\",\n",
    "                     encoding_method=\"OneHot_drop_first\", # This is important for non-binarized data\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.catalog = {'target': dataset.target, 'continuous': dataset.continuous, 'categorical': dataset.categorical, 'immutable': dataset.immutables}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balance on test set 0.23911441129098304, balance on test set 0.23978380149045941\n",
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nr/samba/user/anr/anaconda3/envs/carla_github/lib/python3.7/site-packages/carla/models/catalog/ANN_TORCH/model_ann.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4603 Acc: 0.7729\n",
      "\n",
      "test Loss: 0.3871 Acc: 0.8240\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 0.3732 Acc: 0.8264\n",
      "\n",
      "test Loss: 0.3602 Acc: 0.8327\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 0.3576 Acc: 0.8316\n",
      "\n",
      "test Loss: 0.3496 Acc: 0.8386\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 0.3496 Acc: 0.8345\n",
      "\n",
      "test Loss: 0.3426 Acc: 0.8386\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.3419 Acc: 0.8390\n",
      "\n",
      "test Loss: 0.3364 Acc: 0.8446\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.3385 Acc: 0.8418\n",
      "\n",
      "test Loss: 0.3327 Acc: 0.8464\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.3338 Acc: 0.8432\n",
      "\n",
      "test Loss: 0.3288 Acc: 0.8491\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.3310 Acc: 0.8446\n",
      "\n",
      "test Loss: 0.3269 Acc: 0.8494\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.3290 Acc: 0.8454\n",
      "\n",
      "test Loss: 0.3405 Acc: 0.8419\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.3267 Acc: 0.8474\n",
      "\n",
      "test Loss: 0.3236 Acc: 0.8499\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.3256 Acc: 0.8476\n",
      "\n",
      "test Loss: 0.3213 Acc: 0.8533\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.3244 Acc: 0.8476\n",
      "\n",
      "test Loss: 0.3205 Acc: 0.8529\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.3220 Acc: 0.8486\n",
      "\n",
      "test Loss: 0.3211 Acc: 0.8527\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.3231 Acc: 0.8491\n",
      "\n",
      "test Loss: 0.3186 Acc: 0.8538\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.3216 Acc: 0.8478\n",
      "\n",
      "test Loss: 0.3187 Acc: 0.8537\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.3201 Acc: 0.8497\n",
      "\n",
      "test Loss: 0.3335 Acc: 0.8471\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.3209 Acc: 0.8486\n",
      "\n",
      "test Loss: 0.3171 Acc: 0.8535\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.3188 Acc: 0.8500\n",
      "\n",
      "test Loss: 0.3215 Acc: 0.8515\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.3208 Acc: 0.8496\n",
      "\n",
      "test Loss: 0.3173 Acc: 0.8531\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.3196 Acc: 0.8495\n",
      "\n",
      "test Loss: 0.3174 Acc: 0.8536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from carla.models.catalog import MLModelCatalog\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "ml_model = MLModelCatalog(\n",
    "        dataset, \n",
    "        model_type=\"ann\", \n",
    "        load_online=False, \n",
    "        backend=\"pytorch\"\n",
    "    )\n",
    "\n",
    "ml_model.train(\n",
    "learning_rate=0.002,\n",
    "epochs=20,\n",
    "batch_size=1024,\n",
    "hidden_size=[18, 9, 3],\n",
    "force_train=True, # don't forget to add this or it might load an older model from disk\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9071588643439532"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pred = ml_model.predict_proba(dataset.df_test)\n",
    "pred = [row[1] for row in pred]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(dataset.df_test[dataset.target], pred, pos_label=1)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for MCCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from carla.models.negative_instances import predict_negative_instances\n",
    "\n",
    "factuals = predict_negative_instances(ml_model, dataset.df)\n",
    "test_factual = factuals.iloc[:n_test]\n",
    "\n",
    "y_col = dataset.target\n",
    "cont_feat = dataset.continuous\n",
    "\n",
    "cat_feat = dataset.categorical\n",
    "cat_feat_encoded = dataset.encoder.get_feature_names(dataset.categorical)\n",
    "\n",
    "fixed_features = ['age', 'sex_1']\n",
    "\n",
    "#  Create dtypes for MCCE()\n",
    "dtypes = dict([(x, \"float\") for x in cont_feat])\n",
    "for x in cat_feat_encoded:\n",
    "    dtypes[x] = \"category\"\n",
    "df = (dataset.df).astype(dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit MCCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start = time.time()\n",
    "# # fixed_features = names in dataset\n",
    "# # categorical = original feature names\n",
    "\n",
    "# mcce = MCCE(fixed_features=fixed_features, continuous=dataset.continuous, categorical=dataset.categorical,\\\n",
    "#             model=ml_model, seed=1, catalog=dataset.catalog)\n",
    "\n",
    "# mcce.fit(df.drop(y_col, axis=1), dtypes)\n",
    "\n",
    "# synth_df = mcce.generate(test_factual.drop(y_col, axis=1), k=K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.74\n",
      "1.190302474835284\n",
      "0.14752682721875376\n",
      "0.0\n",
      "1.0\n",
      "100\n",
      "1131.3876264095304\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "K = 10000\n",
    "n_test = 100\n",
    "results_inverse = pd.read_csv(f\"Results/adult_mcce_results_raw_data_k_{K}_n_{n_test}_inverse_transform.csv\", index_col=0)\n",
    "\n",
    "print(results_inverse.L0.mean())\n",
    "print(results_inverse.L2.mean())\n",
    "print(results_inverse.feasibility.mean())\n",
    "print(results_inverse.violation.mean())\n",
    "print(results_inverse.success.mean())\n",
    "print(results_inverse.shape[0])\n",
    "print(results_inverse['time (seconds)'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_raw = pd.read_csv(f\"Results/adult_raw_data_n_{n_test}.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>...</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>relationship</th>\n",
       "      <th>sex</th>\n",
       "      <th>workclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MCCE</td>\n",
       "      <td>50.0</td>\n",
       "      <td>65408.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Original</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83311.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MCCE</td>\n",
       "      <td>20.0</td>\n",
       "      <td>273905.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>34095.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Original</td>\n",
       "      <td>20.0</td>\n",
       "      <td>266015.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>MCCE</td>\n",
       "      <td>30.0</td>\n",
       "      <td>349148.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13550.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Original</td>\n",
       "      <td>30.0</td>\n",
       "      <td>77143.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>MCCE</td>\n",
       "      <td>19.0</td>\n",
       "      <td>247679.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34095.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Original</td>\n",
       "      <td>19.0</td>\n",
       "      <td>301606.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       method   age    fnlwgt  education-num  capital-gain  ...  occupation  \\\n",
       "1        MCCE  50.0   65408.0           13.0           0.0  ...           2   \n",
       "1    Original  50.0   83311.0           13.0           0.0  ...           2   \n",
       "31       MCCE  20.0  273905.0            9.0       34095.0  ...           3   \n",
       "31   Original  20.0  266015.0           10.0           0.0  ...           3   \n",
       "122      MCCE  30.0  349148.0           13.0       13550.0  ...           0   \n",
       "122  Original  30.0   77143.0           13.0           0.0  ...           2   \n",
       "124      MCCE  19.0  247679.0           10.0       34095.0  ...           3   \n",
       "124  Original  19.0  301606.0           10.0           0.0  ...           3   \n",
       "\n",
       "     race  relationship  sex  workclass  \n",
       "1       0             0    0          1  \n",
       "1       0             0    0          1  \n",
       "31      0             2    0          0  \n",
       "31      1             2    0          0  \n",
       "122     2             2    0          0  \n",
       "122     1             2    0          0  \n",
       "124     1             2    0          3  \n",
       "124     1             2    0          0  \n",
       "\n",
       "[8 rows x 14 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_inverse['method'] = 'MCCE'\n",
    "true_raw['method'] = 'Original'\n",
    "temp = pd.concat([results_inverse, true_raw], axis=0)\n",
    "\n",
    "cols = ['method', 'age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', \\\n",
    "       'hours-per-week', 'marital-status', 'native-country', \\\n",
    "       'occupation', 'race', 'relationship', 'sex', 'workclass']\n",
    "\n",
    "to_write = temp[cols].loc[[1, 31, 122, 124]].sort_index()\n",
    "to_write.columns = cols\n",
    "# to_write.sort_values(['Method'], inplace=True, ascending=False)\n",
    "\n",
    "to_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = \"Data/adult.data\"\n",
    "test_path = \"Data/adult.test\"\n",
    "train = pd.read_csv(train_path, sep=\", \", header=None, names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
    "test = pd.read_csv(test_path, skiprows=1, sep=\", \", header=None, names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
    "df = pd.concat([train, test], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in [\"workclass\", \"marital-status\", \"occupation\", \"relationship\", \"sex\", \"race\", \"native-country\"]:\n",
    "    d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "    for i, ind in enumerate(d):\n",
    "        if i <= 3:\n",
    "            d[i] = i\n",
    "        else:\n",
    "            d[i] = 3\n",
    "    mapping = d.to_dict()\n",
    "    dct = {v: k for k, v in mapping.items()}\n",
    "\n",
    "    to_write[feature] = [dct[item] for item in to_write[feature]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'marital-status'\n",
    "dct = {'Married-civ-spouse': 'MCS', 'Never-married': 'NM', 'Divorced': 'D', 'Married-AF-spouse': 'MAFS'}\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'native-country'\n",
    "dct = {'United-States': 'US', 'Holand-Netherlands': 'HS'}\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'occupation'\n",
    "dct = {'Exec-managerial': 'EM', 'Armed-Forces': 'AF', 'Prof-specialty': 'P'}\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'race'\n",
    "dct = {'White': 'W', 'Black': 'B', 'Asian-Pac-Islander': 'API'}\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'relationship'\n",
    "dct = {'Husband': 'H', 'Own-child': 'OC'}\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'sex'\n",
    "dct = {'Male': 'M'}\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "feature = 'workclass'\n",
    "dct = {'Self-emp-not-inc': 'SENI', 'Private': 'P', 'Never-worked': 'NW'}\n",
    "to_write[feature] = [dct[item] for item in to_write[feature]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_write.head(1)\n",
    "print(to_write.to_latex(index=False, float_format=\"%.0f\", ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature = 'workclass'\n",
    "# d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "# for i, ind in enumerate(d):\n",
    "#     if i <= 3:\n",
    "#         d[i] = i\n",
    "#     else:\n",
    "#         d[i] = 3\n",
    "\n",
    "# mapping = d.to_dict()\n",
    "# dct = {v: k for k, v in mapping.items()}\n",
    "\n",
    "# to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "# feature = 'marital-status'\n",
    "# d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "# for i, ind in enumerate(d):\n",
    "#     if i <= 3:\n",
    "#         d[i] = i\n",
    "#     else:\n",
    "#         d[i] = 3\n",
    "# mapping = d.to_dict()\n",
    "# dct = {v: k for k, v in mapping.items()}\n",
    "\n",
    "# to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "# feature = 'occupation'\n",
    "# d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "# for i, ind in enumerate(d):\n",
    "#     if i <= 3:\n",
    "#         d[i] = i\n",
    "#     else:\n",
    "#         d[i] = 3\n",
    "# mapping = d.to_dict()\n",
    "# dct = {v: k for k, v in mapping.items()}\n",
    "\n",
    "# to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "# feature = 'relationship'\n",
    "# d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "# for i, ind in enumerate(d):\n",
    "#     if i <= 3:\n",
    "#         d[i] = i\n",
    "#     else:\n",
    "#         d[i] = 3\n",
    "# mapping = d.to_dict()\n",
    "# dct = {v: k for k, v in mapping.items()}\n",
    "\n",
    "# to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "# feature = 'sex'\n",
    "# d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "# for i, ind in enumerate(d):\n",
    "#     if i <= 3:\n",
    "#         d[i] = i\n",
    "#     else:\n",
    "#         d[i] = 3\n",
    "# mapping = d.to_dict()\n",
    "# dct = {v: k for k, v in mapping.items()}\n",
    "\n",
    "# to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "# feature = 'race'\n",
    "# d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "# for i, ind in enumerate(d):\n",
    "#     if i <= 3:\n",
    "#         d[i] = i\n",
    "#     else:\n",
    "#         d[i] = 3\n",
    "# mapping = d.to_dict()\n",
    "# dct = {v: k for k, v in mapping.items()}\n",
    "\n",
    "# to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "# feature = 'native-country'\n",
    "# d = df.groupby([feature]).size().sort_values(ascending=False)\n",
    "# for i, ind in enumerate(d):\n",
    "#     if i <= 3:\n",
    "#         d[i] = i\n",
    "#     else:\n",
    "#         d[i] = 3\n",
    "# mapping = d.to_dict()\n",
    "# dct = {v: k for k, v in mapping.items()}\n",
    "\n",
    "# to_write[feature] = [dct[item] for item in to_write[feature]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_write\n",
    "# feature = 'workclass'\n",
    "# [dct[item] for item in to_write[feature]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature = 'marital-status'\n",
    "# dct = {'Married-civ-spouse': 'MCS', 'Never-married': 'NM', 'Divorced': 'D', 'Married-AF-spouse': 'MAFS'}\n",
    "# to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "# feature = 'native-country'\n",
    "# dct = {'United-States': 'US', 'Holand-Netherlands': 'HS'}\n",
    "# to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "# feature = 'occupation'\n",
    "# dct = {'Exec-managerial': 'EM', 'Armed-Forces': 'AF', 'Prof-specialty': 'P'}\n",
    "# to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "# feature = 'race'\n",
    "# dct = {'White': 'W', 'Black': 'B', 'Asian-Pac-Islander': 'API'}\n",
    "# to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "# feature = 'relationship'\n",
    "# dct = {'Husband': 'H', 'Own-child': 'OC'}\n",
    "# to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "# feature = 'sex'\n",
    "# dct = {'Male': 'M'}\n",
    "# to_write[feature] = [dct[item] for item in to_write[feature]]\n",
    "\n",
    "\n",
    "# feature = 'workclass'\n",
    "# dct = {'Self-emp-not-inc': 'SENI', 'Private': 'P', 'Never-worked': 'NW'}\n",
    "# to_write[feature] = [dct[item] for item in to_write[feature]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_write.head(1)\n",
    "# print(to_write.to_latex(index=False, float_format=\"%.0f\", ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# data = df\n",
    "# synth = synth_df\n",
    "# test = test_factual\n",
    "# response = y_col\n",
    "# inverse_transform = dataset.inverse_transform\n",
    "# cutoff = 0.5\n",
    "# # Predict response of generated data\n",
    "# synth[response] = ml_model.predict(synth)\n",
    "# synth_positive = synth[synth[response]>=cutoff] # drop negative responses\n",
    "\n",
    "# # Duplicate original test observations N times where N is number of positive counterfactuals\n",
    "# n_counterfactuals = synth_positive.groupby(synth_positive.index).size()\n",
    "# n_counterfactuals = pd.DataFrame(n_counterfactuals, columns = ['N'])\n",
    "\n",
    "# test_repeated = test.copy()\n",
    "\n",
    "# test_repeated = test_repeated.join(n_counterfactuals)\n",
    "# test_repeated.dropna(inplace = True)\n",
    "\n",
    "# test_repeated = test_repeated.reindex(test_repeated.index.repeat(test_repeated.N))\n",
    "# test_repeated.drop(['N'], axis=1, inplace=True)\n",
    "\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# synth=synth_positive\n",
    "# test=test_repeated\n",
    "\n",
    "# features = synth.columns.to_list()\n",
    "# features.remove(response)\n",
    "\n",
    "# synth_metrics = synth.copy()\n",
    "# synth.sort_index(inplace=True)\n",
    "\n",
    "# cols = data.columns\n",
    "# cols.drop(response)\n",
    "\n",
    "# feas_results = []\n",
    "# nbrs = NearestNeighbors(n_neighbors=5).fit(synth[cols].values)\n",
    "\n",
    "# for i, row in synth[cols].iterrows():\n",
    "#     knn = nbrs.kneighbors(row.values.reshape((1, -1)), 5, return_distance=True)[0]\n",
    "    \n",
    "#     feas_results.append(np.mean(knn))\n",
    "\n",
    "# synth_metrics['feasibility'] = feas_results\n",
    "\n",
    "# synth_metrics['success'] = 1\n",
    "\n",
    "# # 6) Success\n",
    "# synth_metrics['success'] = 1\n",
    "# synth.sort_index(inplace=True)\n",
    "\n",
    "# categorical_encoded = []\n",
    "# for x in dataset.df.columns:\n",
    "#     if x not in dataset.continuous:\n",
    "#         if x not in dataset.target:\n",
    "#             categorical_encoded.append(x)\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# len(synth.index.unique())\n",
    "# test.loc[1][features].iloc[1:2]\n",
    "\n",
    "# 1) Distance: Sparsity and Euclidean distance\n",
    "# factual = test[features].sort_index().to_numpy()\n",
    "# counterfactuals = synth[features].sort_index().to_numpy()\n",
    "\n",
    "# cfs_continuous = synth[dataset.continuous].sort_index().to_numpy()\n",
    "# cfs_categorical = synth[categorical_encoded].sort_index().to_numpy()\n",
    "\n",
    "# factual_continuous = test[dataset.continuous].sort_index().to_numpy()\n",
    "# factual_categorical = test[categorical_encoded].sort_index().to_numpy()\n",
    "\n",
    "# delta_cont = factual_continuous - cfs_continuous\n",
    "# delta_cat = factual_categorical - cfs_categorical\n",
    "\n",
    "# delta_cat = np.where(np.abs(delta_cat) > 0, 1, 0)\n",
    "\n",
    "# delta = np.concatenate((delta_cont, delta_cat), axis=1)\n",
    "# d1 = np.sum(np.invert(np.isclose(delta, np.zeros_like(delta), atol=1e-5)), axis=1, dtype=float).tolist() # sparsity\n",
    "# d2 = np.sum(np.abs(delta), axis=1, dtype=float).tolist() # manhatten distance\n",
    "# d3 = np.sum(np.square(np.abs(delta)), axis=1, dtype=np.float).tolist() # euclidean distance\n",
    "\n",
    "# synth_metrics['L0'] = d1\n",
    "# synth_metrics['L1'] = d2\n",
    "# synth_metrics['L2'] = d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_decoded_cfs = pd.DataFrame(scaler.inverse_transform(synth[continuous]), columns=continuous)\n",
    "# df_decoded_cfs.index = synth.index\n",
    "\n",
    "# df_decoded_cfs = pd.concat([df_decoded_cfs, synth[categorical]], axis=1)\n",
    "# df_decoded_cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform(df, continuous, categorical, scaler):\n",
    "#     df_transform = scaler.transform(df[continuous])\n",
    "#     df_transform = pd.DataFrame(df_transform, columns=continuous, index=df.index)\n",
    "#     return pd.concat([df_transform, df[categorical]], axis=1)\n",
    "\n",
    "\n",
    "# def inverse_transform(df, continuous, categorical, scaler):\n",
    "#     df_transform = scaler.inverse_transform(df[continuous])\n",
    "#     df_transform = pd.DataFrame(df_transform, columns=continuous, index=df.index)\n",
    "#     return pd.concat([df_transform, df[categorical]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def intersection(lst1, lst2):\n",
    "#     return list(set(lst1) & set(lst2))\n",
    "\n",
    "\n",
    "# df_decoded_cfs = dataset.inverse_transform(synth)\n",
    "\n",
    "# df_factuals = dataset.inverse_transform(test)\n",
    "\n",
    "# # check continuous using np.isclose to allow for very small numerical differences\n",
    "# cfs_continuous_immutable = df_decoded_cfs[\n",
    "#     intersection(dataset.continuous, fixed_features)\n",
    "# ]\n",
    "\n",
    "# factual_continuous_immutable = df_factuals[\n",
    "#     intersection(dataset.continuous, dataset.immutables)\n",
    "# ]\n",
    "\n",
    "# continuous_violations = np.invert(\n",
    "#     np.isclose(cfs_continuous_immutable, factual_continuous_immutable)\n",
    "# )\n",
    "# continuous_violations = np.sum(continuous_violations, axis=1).reshape(\n",
    "#     (-1, 1)\n",
    "# ) \n",
    "\n",
    "# # check categorical by boolean comparison\n",
    "# cfs_categorical_immutable = df_decoded_cfs[\n",
    "#     intersection(dataset.categorical, dataset.immutables)\n",
    "# ]\n",
    "# # print(cfs_categorical_immutable)\n",
    "# factual_categorical_immutable = df_factuals[\n",
    "#     intersection(dataset.categorical, dataset.immutables)\n",
    "# ]\n",
    "\n",
    "\n",
    "# cfs_categorical_immutable.sort_index(inplace=True)\n",
    "# factual_categorical_immutable.sort_index(inplace=True)\n",
    "# cfs_categorical_immutable.index.name = None\n",
    "\n",
    "# categorical_violations = cfs_categorical_immutable != factual_categorical_immutable\n",
    "# categorical_violations = np.sum(categorical_violations.values, axis=1).reshape(\n",
    "#     (-1, 1)\n",
    "# )  # sum over features\n",
    "\n",
    "# synth_metrics['violation'] = continuous_violations + categorical_violations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = synth_metrics.copy()\n",
    "# results_sparse = pd.DataFrame(columns=results.columns)\n",
    "\n",
    "# for idx in list(set(results.index)):\n",
    "#     idx_df = results.loc[idx]\n",
    "#     if(isinstance(idx_df, pd.DataFrame)): # If you have multiple rows\n",
    "#         sparse = min(idx_df.L0) # 1) find least # features changed\n",
    "#         sparse_df = idx_df[idx_df.L0 == sparse] \n",
    "#         closest = min(sparse_df.L2) # find smallest Gower distance\n",
    "#         close_df = sparse_df[sparse_df.L2 == closest]\n",
    "\n",
    "#         if(close_df.shape[0]>1):\n",
    "#             highest_feasibility = max(close_df.feasibility) #  3) find most feasible\n",
    "#             close_df = close_df[close_df.feasibility == highest_feasibility].head(1)\n",
    "\n",
    "#     else: # if you have only one row - return that row\n",
    "#         close_df = idx_df.to_frame().T\n",
    "        \n",
    "#     results_sparse = pd.concat([results_sparse, close_df], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_sparse[['L0', 'L1', 'L2', 'feasibility', 'violation', 'success']].mean()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea38de303447ace9d448c28089670fa84711b12cac6767c435896f96584513e1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('carla_github')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
